# -*- coding: utf-8 -*-
"""VAE_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10OpBCMMU5Atu5G4dIbB4V1q9KSwmS_RH
"""

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
import torch.utils as utils
from torchvision import datasets, transforms

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def setup_data_loaders(batch_size=128, use_cuda=True):
    root = "../data"
    download=True
    trans = transforms.ToTensor()
    train_set = datasets.MNIST(root=root, train=True, transform=trans, download=download)
    valid_set = datasets.MNIST(root=root, train=False, transform=trans)
    train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)
    valid_loader = torch.utils.data.DataLoader(dataset=valid_set, batch_size=batch_size, shuffle=False)
    return train_loader, valid_loader

class VAE(nn.Module):
    def __init__(self, z_dim, x_dim=28*28):
        super(VAE, self).__init__()
        self.x_dim = x_dim
        self.z_dim = z_dim
        # for encoder
        self.fc1 = nn.Linear(x_dim, 20)
        self.fc2_mean = nn.Linear(20, z_dim)
        self.fc2_var = nn.Linear(20, z_dim)
        # for decoder
        self.fc3 = nn.Linear(z_dim, 20)
        self.fc4 = nn.Linear(20, x_dim)
    
    def encoder(self, x):
        print('encoder--------------------------------')
        print('入力のx', x.shape)
        x = x.view(-1, self.x_dim)
        print('並び替え後のx', x.shape)
        x = F.relu(self.fc1(x))
        print('一つLinearを通した後のx', x.shape)
        mean = self.fc2_mean(x)
        print('mean', mean)
        print('mean', mean.shape)
        log_var = self.fc2_var(x)
        print('log_var', log_var)
        print('log_var', log_var.shape)
        return mean, log_var
    
    # 潜在ベクトルのサンプリング(再パラメータ化)
    def reparametrizaion(self, mean, log_var, device):
        print('reparametrization----------------')
        print('mean', mean)
        print('mean', mean.shape)
        print('log_var', log_var)
        print('log_var', log_var.shape)
        epsilon = torch.randn(mean.shape, device=device)
        print('epsilon', epsilon)
        print('epsilon', epsilon.shape)
        reconst = mean + epsilon*torch.exp(0.5 * log_var)
        print('reconst', reconst.shape)
        return reconst
   
    # デコーダー
    def decoder(self, z):
        print('decoder----------------------------')
        print('zの入力', z)
        print('z', z.shape)
        y = F.relu(self.fc3(z))
        print('y', y.shape)
        y = torch.sigmoid(self.fc4(y)) # 各要素にシグモイド関数を適用し、値を(0,1)の範囲に
        print('y', y.shape)
        return y

    def forward(self, x, device):
        print('forword--------------------------------')
        x = x.view(-1, self.x_dim)
        mean, log_var = self.encoder(x) # 画像xを入力して、平均・分散を出力
        print('forword--------------------------------')
        print('mean', mean.shape)
        print('log_var', log_var.shape)
        KL = 0.5 * torch.sum(1+log_var - mean**2 - torch.exp(log_var)) # KL[q(z|x)||p(z)]を計算
        print('KL', KL)
        print('KL', KL.shape)
        z = self.reparametrizaion(mean, log_var, device) # 潜在ベクトルをサンプリング(再パラメータ化)
        print('forword--------------------------------')
        print('z', z.shape)
        x_hat = self.decoder(z) # 潜在ベクトルを入力して、再構築画像 y を出力
        print('forword--------------------------------')
        print('x_hat', x_hat.shape)
        reconstruction = torch.sum(x * torch.log(x_hat+1e-8) + (1 - x) * torch.log(1 - x_hat  + 1e-8)) #E[log p(x|z)]
        lower_bound = -(KL + reconstruction) #変分下界(ELBO)=E[log p(x|z)] - KL[q(z|x)||p(z)]
        return lower_bound , z, x_hat

dataloader_train, dataloader_valid = setup_data_loaders(batch_size=1000) # データローダーを作成

model = VAE(z_dim = 10).to(device) # モデルをインスタンス化し、GPUにのせる
optimizer = optim.Adam(model.parameters(), lr=1e-3) # オプティマイザーの設定
model.train() # モデルを訓練モードに
num_epochs = 100
loss_list = []
for i in range(num_epochs):
    losses = []
    for x, t in dataloader_train: # データローダーからデータを取り出す。
        x = x.to(device) # データをGPUにのせる
        loss, z, y = model(x, device) # 損失関数の値 loss 、潜在ベクトル z 、再構築画像 y を出力
        model.zero_grad() # モデルの勾配を初期化
        loss.backward() # モデル内のパラメータの勾配を計算
        optimizer.step() # 最適化を実行
        losses.append(loss.cpu().detach().numpy()) # ミニバッチの損失を記録
    loss_list.append(np.average(losses)) # バッチ全体の損失を登録
    print("EPOCH: {} loss: {}".format(i, np.average(losses)))

fig = plt.figure(figsize=(20,4))
model.eval()
zs = []
for x, t in dataloader_valid:
    for i, im in enumerate(x.view(-1,28,28).detach().numpy()[:10]):
        # 元画像を可視化
        ax = fig.add_subplot(2, 10, i+1, xticks=[], yticks=[])
        ax.imshow(im, "gray")
    x = x.to(device)
    _, _, y = model(x, device) #再構築画像 y を出力
    y  = y.view(-1,28,28)
    for i, im in enumerate(y.cpu().detach().numpy()[:10]):
        # 再構築画像を可視化
        ax = fig.add_subplot(2,10,11+i, xticks=[], yticks=[])
        ax.imshow(im, "gray")

fig, ax = plt.subplots(nrows = 3, ncols=5, figsize=(20,12))
model.eval()
for r in range(3):
    for c in range(5):
        ax[r,c].imshow(model.decoder(torch.randn(10).cuda()).detach().cpu().numpy().reshape(28,28), cmap="gray")
        ax[r,c].axis("off")

